{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Python Crash Course_Part 2 Data Preparation\n## Missing, Distinct Value, Scaling\n\n## Full Day Workshop for user learn Data Science with Python\n### 2017 Dec Timothy CL Lam\nThis is meant for internal usage, not for commercial purpose\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "  CHURN Gender Status  Children  Est Income Car Owner        Age  \\\n0     T      F      S       1.0    38000.00         N  24.393333   \n1     F      M      M       2.0    29616.00         N  49.426667   \n2     F      M      M       0.0    19732.80         N  50.673333   \n3     F      M      S       2.0       96.33         N  56.473333   \n4     F      F      M       2.0    52004.80         N  25.140000   \n\n   LongDistance  International   Local  Dropped Paymethod LocalBilltype  \\\n0         23.56            0.0  206.08      0.0        CC        Budget   \n1         29.78            0.0   45.50      0.0        CH     FreeLocal   \n2         24.81            0.0   22.44      0.0        CC     FreeLocal   \n3         26.13            0.0   32.88      1.0        CC        Budget   \n4          5.03            0.0   23.11      0.0        CH        Budget   \n\n  LongDistanceBilltype   Usage  RatePlan  \n0       Intnl_discount  229.64       3.0  \n1             Standard   75.29       2.0  \n2             Standard   47.25       3.0  \n3             Standard   59.01       1.0  \n4       Intnl_discount   28.14       1.0  ", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CHURN</th>\n      <th>Gender</th>\n      <th>Status</th>\n      <th>Children</th>\n      <th>Est Income</th>\n      <th>Car Owner</th>\n      <th>Age</th>\n      <th>LongDistance</th>\n      <th>International</th>\n      <th>Local</th>\n      <th>Dropped</th>\n      <th>Paymethod</th>\n      <th>LocalBilltype</th>\n      <th>LongDistanceBilltype</th>\n      <th>Usage</th>\n      <th>RatePlan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T</td>\n      <td>F</td>\n      <td>S</td>\n      <td>1.0</td>\n      <td>38000.00</td>\n      <td>N</td>\n      <td>24.393333</td>\n      <td>23.56</td>\n      <td>0.0</td>\n      <td>206.08</td>\n      <td>0.0</td>\n      <td>CC</td>\n      <td>Budget</td>\n      <td>Intnl_discount</td>\n      <td>229.64</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>F</td>\n      <td>M</td>\n      <td>M</td>\n      <td>2.0</td>\n      <td>29616.00</td>\n      <td>N</td>\n      <td>49.426667</td>\n      <td>29.78</td>\n      <td>0.0</td>\n      <td>45.50</td>\n      <td>0.0</td>\n      <td>CH</td>\n      <td>FreeLocal</td>\n      <td>Standard</td>\n      <td>75.29</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>F</td>\n      <td>M</td>\n      <td>M</td>\n      <td>0.0</td>\n      <td>19732.80</td>\n      <td>N</td>\n      <td>50.673333</td>\n      <td>24.81</td>\n      <td>0.0</td>\n      <td>22.44</td>\n      <td>0.0</td>\n      <td>CC</td>\n      <td>FreeLocal</td>\n      <td>Standard</td>\n      <td>47.25</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F</td>\n      <td>M</td>\n      <td>S</td>\n      <td>2.0</td>\n      <td>96.33</td>\n      <td>N</td>\n      <td>56.473333</td>\n      <td>26.13</td>\n      <td>0.0</td>\n      <td>32.88</td>\n      <td>1.0</td>\n      <td>CC</td>\n      <td>Budget</td>\n      <td>Standard</td>\n      <td>59.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>F</td>\n      <td>F</td>\n      <td>M</td>\n      <td>2.0</td>\n      <td>52004.80</td>\n      <td>N</td>\n      <td>25.140000</td>\n      <td>5.03</td>\n      <td>0.0</td>\n      <td>23.11</td>\n      <td>0.0</td>\n      <td>CH</td>\n      <td>Budget</td>\n      <td>Intnl_discount</td>\n      <td>28.14</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 86, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 87, 
            "cell_type": "code", 
            "source": "import pandas as pd\nimport numpy as np\nfrom numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import defaultdict, Counter", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 88, 
            "cell_type": "code", 
            "source": "numerical_columns = list(df1.select_dtypes(include=[np.number]).columns)\ncategorical_columns = list(df1.select_dtypes(include=[object]).columns)\ndate_columns = list(df1.select_dtypes(include=['<M8[ns]']).columns)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 89, 
            "cell_type": "code", 
            "source": "categorical_columns[:]", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "['CHURN',\n 'Gender',\n 'Status',\n 'Car Owner',\n 'Paymethod',\n 'LocalBilltype',\n 'LongDistanceBilltype']"
                    }, 
                    "execution_count": 89, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 90, 
            "cell_type": "code", 
            "source": "print \"Loaded dataset\"\nprint \"   Rows: %s\" % df1.shape[0]\nprint \"   Columns: %s (%s num, %s cat, %s date)\" % (df1.shape[1], \n                                                    len(numerical_columns), len(categorical_columns),\n                                                    len(date_columns))", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Loaded dataset\n   Rows: 2066\n   Columns: 16 (9 num, 7 cat, 0 date)\n"
                }
            ]
        }, 
        {
            "source": "## Additional preparation\n### Too Many Unique Values\nGet rid of the columns that contain too many unique values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 91, 
            "cell_type": "code", 
            "source": "DROP_LIMIT_ABS = 200\nCAT_DROP_LIMIT_RATIO = 0.5\nfor feature in categorical_columns:\n    nu = df1[feature].nunique()\n    \n    if nu > DROP_LIMIT_ABS or nu > CAT_DROP_LIMIT_RATIO*df1.shape[0]:\n        print \"Dropping feature %s with %s values\" % (feature, nu)\n        columns_to_drop.append(feature)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "### Missing Value Analysis\nWe then need to impute missing values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 92, 
            "cell_type": "code", 
            "source": "# Use mean for numerical features\nfor feature in numerical_columns:\n    v = df1[feature].mean()\n    if np.isnan(v):\n        v = 0\n    print \"Filling %s with %s\" % (feature, v)\n    df1[feature] = df1[feature].fillna(v)\n    \n# Use mode for categorical features\nfor feature in categorical_columns:\n    v = df1[feature].value_counts().index[0]\n    df1[feature] = df1[feature].fillna(v)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Filling Children with 1.14617618587\nFilling Est Income with 51514.0704647\nFilling Age with 42.7839819342\nFilling LongDistance with 16.1220764763\nFilling International with 1.1911035818\nFilling Local with 59.1580251694\nFilling Dropped with 0.136011616651\nFilling Usage with 75.907696031\nFilling RatePlan with 2.51064859632\n"
                }
            ]
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "source": "# Change sample size here\ndataset_limit = 10000\n# Toggle to keep dates in your dataset\nkeep_dates = False\n# Toggle to dummify categorical features\ndummify_categories = False\n\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "By default, date features are not kept. Modify the following cell to change that.\n\nAlso, by default, categorical columns are not included in the t-SNE data (preventing the creation of very sparse data). Modify the following cell to enable dummification.\nWe also use a categorical column for coloring if possible.\n\nKeep the dates as features if requested by the user", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "source": "columns_to_drop = []\n\nif keep_dates:\n    df[date_columns] = df[date_columns].astype(int)*1e-9\nelse:\n    columns_to_drop.extend(date_columns)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "## Dummy encoding\nFor all categorical features, we are going to \"dummy-encode\" them (also sometimes called one-hot encoding).\n\nBasically, a categorical feature is replaced by one column per value. Each created value contains 0 or 1 depending on whether the original value was the one of the column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 95, 
            "cell_type": "code", 
            "source": "# For categorical variables with more than that many values, we only keep the most frequent ones\nLIMIT_DUMMIES = 100\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values = {}\n    for feature in features:\n        values = [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] = values\n    return dummy_values\n\nDUMMY_VALUES = select_dummy_values(df1, [x for x in categorical_columns if not x in columns_to_drop])\n\n\ndef dummy_encode_dataframe(df1):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name = u'%s_value_%s' % (feature, dummy_value.decode('utf-8'))\n            df1[dummy_name] = (df1[feature] == dummy_value).astype(float)\n        del df1[feature]\n        print 'Dummy-encoded feature %s' % feature\n\ndummy_encode_dataframe(df1)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Dummy-encoded feature Status\nDummy-encoded feature Paymethod\nDummy-encoded feature Gender\nDummy-encoded feature Car Owner\nDummy-encoded feature LongDistanceBilltype\nDummy-encoded feature CHURN\nDummy-encoded feature LocalBilltype\n"
                }
            ]
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "source": "df1.head()", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "   Children  Est Income        Age  LongDistance  International   Local  \\\n0       1.0    38000.00  24.393333         23.56            0.0  206.08   \n1       2.0    29616.00  49.426667         29.78            0.0   45.50   \n2       0.0    19732.80  50.673333         24.81            0.0   22.44   \n3       2.0       96.33  56.473333         26.13            0.0   32.88   \n4       2.0    52004.80  25.140000          5.03            0.0   23.11   \n\n   Dropped   Usage  RatePlan  Status_value_M              ...                \\\n0      0.0  229.64       3.0             0.0              ...                 \n1      0.0   75.29       2.0             1.0              ...                 \n2      0.0   47.25       3.0             1.0              ...                 \n3      1.0   59.01       1.0             0.0              ...                 \n4      0.0   28.14       1.0             1.0              ...                 \n\n   Gender_value_F  Gender_value_M  Car Owner_value_N  Car Owner_value_Y  \\\n0             1.0             0.0                1.0                0.0   \n1             0.0             1.0                1.0                0.0   \n2             0.0             1.0                1.0                0.0   \n3             0.0             1.0                1.0                0.0   \n4             1.0             0.0                1.0                0.0   \n\n   LongDistanceBilltype_value_Standard  \\\n0                                  0.0   \n1                                  1.0   \n2                                  1.0   \n3                                  1.0   \n4                                  0.0   \n\n   LongDistanceBilltype_value_Intnl_discount  CHURN_value_F  CHURN_value_T  \\\n0                                        1.0            0.0            1.0   \n1                                        0.0            1.0            0.0   \n2                                        0.0            1.0            0.0   \n3                                        0.0            1.0            0.0   \n4                                        1.0            1.0            0.0   \n\n   LocalBilltype_value_Budget  LocalBilltype_value_FreeLocal  \n0                         1.0                            0.0  \n1                         0.0                            1.0  \n2                         0.0                            1.0  \n3                         1.0                            0.0  \n4                         1.0                            0.0  \n\n[5 rows x 25 columns]", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Children</th>\n      <th>Est Income</th>\n      <th>Age</th>\n      <th>LongDistance</th>\n      <th>International</th>\n      <th>Local</th>\n      <th>Dropped</th>\n      <th>Usage</th>\n      <th>RatePlan</th>\n      <th>Status_value_M</th>\n      <th>...</th>\n      <th>Gender_value_F</th>\n      <th>Gender_value_M</th>\n      <th>Car Owner_value_N</th>\n      <th>Car Owner_value_Y</th>\n      <th>LongDistanceBilltype_value_Standard</th>\n      <th>LongDistanceBilltype_value_Intnl_discount</th>\n      <th>CHURN_value_F</th>\n      <th>CHURN_value_T</th>\n      <th>LocalBilltype_value_Budget</th>\n      <th>LocalBilltype_value_FreeLocal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>38000.00</td>\n      <td>24.393333</td>\n      <td>23.56</td>\n      <td>0.0</td>\n      <td>206.08</td>\n      <td>0.0</td>\n      <td>229.64</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>29616.00</td>\n      <td>49.426667</td>\n      <td>29.78</td>\n      <td>0.0</td>\n      <td>45.50</td>\n      <td>0.0</td>\n      <td>75.29</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>19732.80</td>\n      <td>50.673333</td>\n      <td>24.81</td>\n      <td>0.0</td>\n      <td>22.44</td>\n      <td>0.0</td>\n      <td>47.25</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>96.33</td>\n      <td>56.473333</td>\n      <td>26.13</td>\n      <td>0.0</td>\n      <td>32.88</td>\n      <td>1.0</td>\n      <td>59.01</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>52004.80</td>\n      <td>25.140000</td>\n      <td>5.03</td>\n      <td>0.0</td>\n      <td>23.11</td>\n      <td>0.0</td>\n      <td>28.14</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 25 columns</p>\n</div>"
                    }, 
                    "execution_count": 96, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "### There are number of Scalers. Please find the difference from here:\nhttp://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### MinMaxScaler\nOften this is referred toas normalization and attributes are often rescaled into the range between 0 and 1. This is\nuseful for optimization algorithms used in the core of machine learning algorithms like gradient\ndescent. It is also useful for algorithms that weight inputs like regression and neural networks\nand algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data\nusing scikit-learn using the MinMaxScaler class2.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 102, 
            "cell_type": "code", 
            "source": "X = df1.values\n    \nfrom sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler().fit(X)\nX_std = mm.transform(X)\nX_std.shape\nX_std[:]", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 0.5  ,  0.316,  0.187, ...,  1.   ,  1.   ,  0.   ],\n       [ 1.   ,  0.246,  0.574, ...,  0.   ,  0.   ,  1.   ],\n       [ 0.   ,  0.164,  0.593, ...,  0.   ,  0.   ,  1.   ],\n       ..., \n       [ 0.   ,  0.699,  0.753, ...,  1.   ,  1.   ,  0.   ],\n       [ 1.   ,  0.235,  0.409, ...,  1.   ,  0.   ,  1.   ],\n       [ 0.   ,  0.238,  0.051, ...,  1.   ,  0.   ,  1.   ]])"
                    }, 
                    "execution_count": 102, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "### StandardScaler\nStandardization is a useful technique to transform attributes with a Gaussian distribution and\ndi\u000bering means and standard deviations to a standard Gaussian distribution with a mean of\n0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian\ndistribution in the input variables and work better with rescaled data, such as linear regression,\nlogistic regression and linear discriminate analysis. You can standardize data using scikit-learn\nwith the StandardScaler class3.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 103, 
            "cell_type": "code", 
            "source": "Y = df1.values\n    \nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler().fit(Y)\nY_std = ss.transform(Y)\nY_std.shape\nY_std[:]\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[-0.173, -0.439, -1.235, ...,  1.207,  0.914, -0.914],\n       [ 1.013, -0.711,  0.446, ..., -0.829, -1.094,  1.094],\n       [-1.36 , -1.032,  0.53 , ..., -0.829, -1.094,  1.094],\n       ..., \n       [-1.36 ,  1.051,  1.225, ...,  1.207,  0.914, -0.914],\n       [ 1.013, -0.756, -0.27 , ...,  1.207, -1.094,  1.094],\n       [-1.36 , -0.744, -1.826, ...,  1.207, -1.094,  1.094]])"
                    }, 
                    "execution_count": 103, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "### RobustScaler\nUnlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (see below).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 107, 
            "cell_type": "code", 
            "source": "Z = df1.values\n    \nfrom sklearn.preprocessing import RobustScaler\nrs = RobustScaler(quantile_range=(25, 75)).fit(Z)\nZ_std = rs.transform(Z)\nZ_std.shape\nZ_std[:]\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 0.   , -0.313, -0.893, ...,  1.   ,  0.   ,  0.   ],\n       [ 0.5  , -0.461,  0.165, ...,  0.   , -1.   ,  1.   ],\n       [-0.5  , -0.634,  0.218, ...,  0.   , -1.   ,  1.   ],\n       ..., \n       [-0.5  ,  0.492,  0.655, ...,  1.   ,  0.   ,  0.   ],\n       [ 0.5  , -0.485, -0.286, ...,  1.   , -1.   ,  1.   ],\n       [-0.5  , -0.479, -1.265, ...,  1.   , -1.   ,  1.   ]])"
                    }, 
                    "execution_count": 107, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "### QuantileTransformer\nQuantileTransformer applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform distribution. In this case, all the data will be mapped in the range [0, 1], even the outliers which cannot be distinguished anymore from the inliers.\nAs RobustScaler, QuantileTransformer is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to RobustScaler, QuantileTransformer will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1).\n\nQuantileTransformer has an additional output_distribution parameter allowing to match a Gaussian distribution instead of a uniform distribution. Note that this non-parametetric transformer introduces saturation artifacts for extreme values.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 108, 
            "cell_type": "code", 
            "source": "Q = df1.values\n    \nfrom sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(output_distribution='normal').fit(Q)\nQ_std = qt.transform(Q)\nQ_std.shape\nQ_std[:]\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[-0.184, -0.351, -0.926, ...,  5.199,  5.199, -5.199],\n       [ 5.199, -0.491,  0.303, ..., -5.199, -5.199,  5.199],\n       [-5.199, -0.763,  0.363, ..., -5.199, -5.199,  5.199],\n       ..., \n       [-5.199,  0.848,  1.245, ...,  5.199,  5.199, -5.199],\n       [ 5.199, -0.56 , -0.432, ...,  5.199, -5.199,  5.199],\n       [-5.199, -0.534, -1.733, ...,  5.199, -5.199,  5.199]])"
                    }, 
                    "execution_count": 108, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }
    ], 
    "metadata": {
        "language_info": {
            "version": "2.7.11", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat_minor": 1
}
