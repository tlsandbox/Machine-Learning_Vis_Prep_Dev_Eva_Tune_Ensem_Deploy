{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Python Crash Course Part 4 Build & Optimize Models\n### This is meant for internal usage, not for commercial purpose\n### Timothy CL Lam 2017 DEC", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Let's talk about some algorithms", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Classification / Linear Algorithms", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Logistic Regression:\nLogistic Regression\nLogistic regression assumes a Gaussian distribution for the numeric input variables and can\nmodel binary classi\fcation problems. You can construct a logistic regression model using the\nLogisticRegression class1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Linear Discriminant Analysis or LDA\nis a statistical technique for binary and multiclass\nclassi\fcation. It too assumes a Gaussian distribution for the numerical input variables. You can\nconstruct an LDA model using the LinearDiscriminantAnalysis class2.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Classication / Non-linear Algorithms", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### k-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) uses a distance metric to \fnd the k most similar\ninstances in the training data for a new instance and takes the mean outcome of the neighbors\nas the prediction. You can construct a KNN model using the KNeighborsClassifier class3.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Naive Bayes\nNaive Bayes calculates the probability of each class and the conditional probability of each class\ngiven each input value. These probabilities are estimated for new data and multiplied together,\nassuming that they are all independent (a simple or naive assumption). When working with\nreal-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\ninput variables using the Gaussian Probability Density Function. You can construct a Naive\nBayes model using the GaussianNB class4.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Classi\fcation and Regression Trees\nClassi\fcation and Regression Trees (CART or just decision trees) construct a binary tree from\nthe training data. Split points are chosen greedily by evaluating each attribute and each value\nof each attribute in the training data in order to minimize a cost function (like the Gini index).\nYou can construct a CART model using the DecisionTreeClassifier class5.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Support Vector Machines\nSupport Vector Machines (or SVM) seek a line that best separates two classes. Those data\ninstances that are closest to the line that best separates the classes are called support vectors\nand in\nuence where the line is placed. SVM has been extended to support multiple classes.\nOf particular importance is the use of di\u000berent kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default. You can construct an SVM model using the\nSVC class6.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Regression / Linear Algorithms", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Ridge Regression\nRidge regression is an extension of linear regression where the loss function is modi\fed to\nminimize the complexity of the model measured as the sum squared value of the coe\u000ecient\nvalues (also called the L2-norm). You can construct a ridge regression model by using the Ridge\nclass2", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### LASSO Regression\nThe Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modi\fcation\nof linear regression, like ridge regression, where the loss function is modi\fed to minimize the\ncomplexity of the model measured as the sum absolute value of the coe\u000ecient values (also called\nthe L1-norm). You can construct a LASSO model by using the Lasso class3.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### ElasticNet Regression\nElasticNet is a form of regularization regression that combines the properties of both Ridge\nRegression and LASSO regression. It seeks to minimize the complexity of the regression model\n(magnitude and number of regression coe\u000ecients) by penalizing the model using both the\nL2-norm (sum squared coe\u000ecient values) and the L1-norm (sum absolute coe\u000ecient values).\nYou can construct an ElasticNet model using the ElasticNet class4.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Non-Linear Algorithms will be same as Classification e.g. SVM, Regression Tree, KNN", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Let's start coding!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom collections import defaultdict, Counter\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "   CUSTOMER_ID  APPROVABLE    COLLEGE RESIDENTIAL  MAX_NUM_OVERDUES_1_YEAR  \\\n0      34274.0         0.0  Bachelors       Other                      0.0   \n1     174240.0         0.0       None        Rent                      0.0   \n2       9434.0         1.0       None        Rent                      0.0   \n3      69251.0         1.0       None         Own                      0.0   \n4      39518.0         1.0  Bachelors        Rent                      0.0   \n\n   MONTHS_SINCE_60DAY_OVER  AVG_OVERDRAFT_BALANCE  AVG_CREDIT_UTILIZATION  \\\n0                      0.0            1979.166812                0.485324   \n1                     13.0            1335.582937                0.227195   \n2                      0.0            1342.754895                0.631966   \n3                      0.0             894.905748                0.548344   \n4                      0.0             923.309535                0.404981   \n\n   DEBT_INCOME_RATIO  MONTHS_CREDIT_ESTABLISH  CREDIT_QUERIES_6MONTHS  \\\n0           0.479821                    106.0                    11.0   \n1           0.478645                     79.0                     5.0   \n2           0.275831                     80.0                     2.0   \n3           0.314404                     94.0                     3.0   \n4           0.452962                     67.0                     3.0   \n\n   NUM_CREDIT_CARDS      LOAN_AMT       APR  \n0               5.0  448180.89050  0.089636  \n1               3.0  314995.97575  0.062999  \n2               4.0  254888.93225  0.050978  \n3               1.0  271183.56000  0.054237  \n4               3.0  271242.51900  0.054249  ", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_ID</th>\n      <th>APPROVABLE</th>\n      <th>COLLEGE</th>\n      <th>RESIDENTIAL</th>\n      <th>MAX_NUM_OVERDUES_1_YEAR</th>\n      <th>MONTHS_SINCE_60DAY_OVER</th>\n      <th>AVG_OVERDRAFT_BALANCE</th>\n      <th>AVG_CREDIT_UTILIZATION</th>\n      <th>DEBT_INCOME_RATIO</th>\n      <th>MONTHS_CREDIT_ESTABLISH</th>\n      <th>CREDIT_QUERIES_6MONTHS</th>\n      <th>NUM_CREDIT_CARDS</th>\n      <th>LOAN_AMT</th>\n      <th>APR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34274.0</td>\n      <td>0.0</td>\n      <td>Bachelors</td>\n      <td>Other</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1979.166812</td>\n      <td>0.485324</td>\n      <td>0.479821</td>\n      <td>106.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>448180.89050</td>\n      <td>0.089636</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>174240.0</td>\n      <td>0.0</td>\n      <td>None</td>\n      <td>Rent</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>1335.582937</td>\n      <td>0.227195</td>\n      <td>0.478645</td>\n      <td>79.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>314995.97575</td>\n      <td>0.062999</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9434.0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>Rent</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1342.754895</td>\n      <td>0.631966</td>\n      <td>0.275831</td>\n      <td>80.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>254888.93225</td>\n      <td>0.050978</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69251.0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>Own</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>894.905748</td>\n      <td>0.548344</td>\n      <td>0.314404</td>\n      <td>94.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>271183.56000</td>\n      <td>0.054237</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>39518.0</td>\n      <td>1.0</td>\n      <td>Bachelors</td>\n      <td>Rent</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>923.309535</td>\n      <td>0.404981</td>\n      <td>0.452962</td>\n      <td>67.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>271242.51900</td>\n      <td>0.054249</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 19, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "source": "numerical_columns = list(df1.select_dtypes(include=[np.number]).columns)\ncategorical_columns = list(df1.select_dtypes(include=[object]).columns)\ndate_columns = list(df1.select_dtypes(include=['<M8[ns]']).columns)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "source": "#One Hot Encoder. Transform catogrical variable into numeric one\ncolumns_to_drop = []\n\n# For categorical variables with more than that many values, we only keep the most frequent ones\nLIMIT_DUMMIES = 100\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values = {}\n    for feature in features:\n        values = [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] = values\n    return dummy_values\n\nDUMMY_VALUES = select_dummy_values(df1, [x for x in categorical_columns if not x in columns_to_drop])\n\n\ndef dummy_encode_dataframe(df1):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name = u'%s_value_%s' % (feature, dummy_value.decode('utf-8'))\n            df1[dummy_name] = (df1[feature] == dummy_value).astype(float)\n        del df1[feature]\n        print 'Dummy-encoded feature %s' % feature\n\ndummy_encode_dataframe(df1)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Dummy-encoded feature RESIDENTIAL\nDummy-encoded feature COLLEGE\n"
                }
            ]
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "source": "#Drop Customer ID\ndf1.drop('CUSTOMER_ID', axis=1, inplace=True)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "source": "df1.info()", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10005 entries, 0 to 10004\nData columns (total 17 columns):\nAPPROVABLE                 10005 non-null float64\nMAX_NUM_OVERDUES_1_YEAR    10005 non-null float64\nMONTHS_SINCE_60DAY_OVER    10005 non-null float64\nAVG_OVERDRAFT_BALANCE      10005 non-null float64\nAVG_CREDIT_UTILIZATION     10005 non-null float64\nDEBT_INCOME_RATIO          10005 non-null float64\nMONTHS_CREDIT_ESTABLISH    10005 non-null float64\nCREDIT_QUERIES_6MONTHS     10005 non-null float64\nNUM_CREDIT_CARDS           10005 non-null float64\nLOAN_AMT                   10005 non-null float64\nAPR                        10005 non-null float64\nRESIDENTIAL_value_Own      10005 non-null float64\nRESIDENTIAL_value_Rent     10005 non-null float64\nRESIDENTIAL_value_Other    10005 non-null float64\nCOLLEGE_value_None         10005 non-null float64\nCOLLEGE_value_Bachelors    10005 non-null float64\nCOLLEGE_value_Advanced     10005 non-null float64\ndtypes: float64(17)\nmemory usage: 1.3 MB\n"
                }
            ]
        }, 
        {
            "execution_count": 47, 
            "cell_type": "code", 
            "source": "#Scale the Value for Model Building\nfrom sklearn.preprocessing import RobustScaler\narray = df1.values    \nrs = RobustScaler(quantile_range=(25, 75)).fit(array)\nS_array = rs.transform(array)\nS_array.shape\nS_array[:]\ndataframe=pd.DataFrame(S_array)\ndataframe.head()", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "    0    1     2         3         4         5         6     7         8   \\\n0 -1.0  0.0   0.0  0.492173 -0.061694  0.196863  0.446809  1.50  1.000000   \n1 -1.0  0.0  13.0 -0.135871 -0.585781  0.191754 -0.127660  0.00  0.333333   \n2  0.0  0.0   0.0 -0.128872  0.236035 -0.689911 -0.106383 -0.75  0.666667   \n3  0.0  0.0   0.0 -0.565907  0.066256 -0.522228  0.191489 -0.50 -0.333333   \n4  0.0  0.0   0.0 -0.538189 -0.224817  0.080105 -0.382979 -0.50  0.333333   \n\n         9         10   11   12   13   14   15   16  \n0  0.593728  0.593728  0.0  0.0  1.0 -1.0  1.0  0.0  \n1 -0.481966 -0.481966  0.0  1.0  0.0  0.0  0.0  0.0  \n2 -0.967432 -0.967432  0.0  1.0  0.0  0.0  0.0  0.0  \n3 -0.835825 -0.835825  1.0  0.0  0.0  0.0  0.0  0.0  \n4 -0.835349 -0.835349  0.0  1.0  0.0 -1.0  1.0  0.0  ", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.492173</td>\n      <td>-0.061694</td>\n      <td>0.196863</td>\n      <td>0.446809</td>\n      <td>1.50</td>\n      <td>1.000000</td>\n      <td>0.593728</td>\n      <td>0.593728</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>-0.135871</td>\n      <td>-0.585781</td>\n      <td>0.191754</td>\n      <td>-0.127660</td>\n      <td>0.00</td>\n      <td>0.333333</td>\n      <td>-0.481966</td>\n      <td>-0.481966</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.128872</td>\n      <td>0.236035</td>\n      <td>-0.689911</td>\n      <td>-0.106383</td>\n      <td>-0.75</td>\n      <td>0.666667</td>\n      <td>-0.967432</td>\n      <td>-0.967432</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.565907</td>\n      <td>0.066256</td>\n      <td>-0.522228</td>\n      <td>0.191489</td>\n      <td>-0.50</td>\n      <td>-0.333333</td>\n      <td>-0.835825</td>\n      <td>-0.835825</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.538189</td>\n      <td>-0.224817</td>\n      <td>0.080105</td>\n      <td>-0.382979</td>\n      <td>-0.50</td>\n      <td>0.333333</td>\n      <td>-0.835349</td>\n      <td>-0.835349</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 47, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "source": "y[:]", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([ 0.,  0.,  1., ...,  1.,  1.,  1.])"
                    }, 
                    "execution_count": 29, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "source": "S_array.info()", 
            "metadata": {}, 
            "outputs": [
                {
                    "evalue": "'numpy.ndarray' object has no attribute 'info'", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-36-204358c6354b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mS_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'info'"
                    ], 
                    "ename": "AttributeError", 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "source": "y [:]", 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([-1., -1.,  0., ...,  0.,  0.,  0.])"
                    }, 
                    "execution_count": 37, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "source": "dataframe=pd.DataFrame(S_array)\nprint (dataframe)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "        0    1     2         3         4         5         6     7         8   \\\n0     -1.0  0.0   0.0  0.492173 -0.061694  0.196863  0.446809  1.50  1.000000   \n1     -1.0  0.0  13.0 -0.135871 -0.585781  0.191754 -0.127660  0.00  0.333333   \n2      0.0  0.0   0.0 -0.128872  0.236035 -0.689911 -0.106383 -0.75  0.666667   \n3      0.0  0.0   0.0 -0.565907  0.066256 -0.522228  0.191489 -0.50 -0.333333   \n4      0.0  0.0   0.0 -0.538189 -0.224817  0.080105 -0.382979 -0.50  0.333333   \n5      0.0  0.0   0.0  0.118614 -0.635693  0.319069 -1.468085  1.00  0.000000   \n6      0.0  0.0   0.0 -0.213953  0.560153 -0.629415 -1.085106 -0.75 -0.333333   \n7      0.0  0.0   0.0 -1.098787 -0.633508 -0.270375 -0.446809  0.00  0.666667   \n8      0.0  1.0   0.0 -0.025477 -0.851087 -0.805405 -0.191489  1.75  0.666667   \n9      0.0  0.0   0.0 -0.209369  0.027925 -0.589277 -0.276596  2.00  0.000000   \n10    -1.0  0.0   0.0 -0.311512 -0.455131 -0.727702 -1.212766 -1.25  0.666667   \n11     0.0  0.0   0.0 -0.611103 -1.022484 -0.290651  0.170213  2.00 -0.333333   \n12    -1.0  0.0   0.0 -1.017562  1.523050  0.467980  0.808511 -1.25 -0.666667   \n13     0.0  0.0   0.0  0.054730 -0.318710 -0.771340 -0.085106  0.75  1.333333   \n14    -1.0  0.0   0.0 -0.581879 -0.014742  0.424790 -0.340426 -1.25 -0.666667   \n15     0.0  0.0   0.0  0.523876  0.018465 -0.757753 -0.340426  0.00 -0.333333   \n16     0.0  0.0   0.0 -0.864792 -0.349073  0.842892  0.468085  0.75  1.333333   \n17     0.0  0.0  15.0  1.402293  0.436479  1.289366  0.127660 -0.75  1.000000   \n18     0.0  0.0   0.0 -0.343486 -0.194703 -0.804737 -0.574468  0.00 -0.333333   \n19     0.0  0.0   9.0 -0.345774 -1.007495  0.455955 -0.617021  0.75 -0.333333   \n20    -1.0  0.0  14.0  0.279954  0.020710 -0.033209  1.085106 -1.25 -0.666667   \n21     0.0  1.0   0.0  0.042602  0.186113 -0.460976 -0.829787 -1.00  1.333333   \n22     0.0  0.0   0.0 -0.655592  1.297284  0.161385 -1.063830  1.25  1.000000   \n23     0.0  0.0   0.0  0.309020  0.526602 -0.364275 -0.851064 -1.00  1.000000   \n24     0.0  0.0   0.0  0.219977  0.258031  0.201570 -0.361702 -0.50 -0.333333   \n25     0.0  0.0   0.0  1.736811  0.039415 -1.137770 -0.021277 -0.50 -0.333333   \n26    -1.0  0.0   0.0 -1.266994 -0.302470 -0.441707 -0.297872  0.00  0.000000   \n27     0.0  0.0   0.0  0.146452 -0.015352 -0.034458 -0.680851  0.00 -0.333333   \n28     0.0  0.0   0.0  0.655564  1.097004  1.288260  1.574468  0.25  0.000000   \n29     0.0  0.0   0.0  0.700901 -0.778073  0.741225 -1.021277 -0.75  1.333333   \n...    ...  ...   ...       ...       ...       ...       ...   ...       ...   \n9975   0.0  0.0   6.0 -0.974092 -0.064863 -0.551013 -0.872340  0.25 -0.333333   \n9976   0.0  0.0   0.0 -0.095750 -0.248099 -0.095675 -0.212766 -1.00  2.333333   \n9977   0.0  0.0   0.0 -0.580670 -0.380372  0.892793  1.787234  0.25  0.000000   \n9978  -1.0  4.0   0.0 -0.054298  0.250802 -0.682024 -0.340426 -0.50 -0.333333   \n9979   0.0  0.0  36.0 -0.138414 -0.539435  0.136441 -0.191489  1.00 -0.333333   \n9980  -1.0  0.0  13.0  0.760849 -0.310159  0.276064 -1.276596  0.25  0.333333   \n9981   0.0  0.0  19.0  1.537388 -0.678386 -0.579938 -1.340426 -0.50 -0.333333   \n9982  -1.0  0.0  33.0  0.087991  0.230210  0.252022  0.787234  0.75 -0.666667   \n9983   0.0  0.0   0.0  1.936458  0.181722  1.226919 -0.063830  1.75  0.000000   \n9984   0.0  0.0   0.0  1.067357  1.291345  2.065421 -0.106383  0.75  2.000000   \n9985   0.0  0.0   0.0 -0.443894  0.374443  1.135492 -0.148936 -0.25 -0.333333   \n9986   0.0  0.0   0.0  0.125145 -0.359117  0.301673 -0.148936 -0.75  1.666667   \n9987  -1.0  1.0   0.0  0.127573 -0.548136 -0.065298 -0.702128  0.25  0.333333   \n9988   0.0  0.0   0.0 -0.149402 -0.162624 -0.450793  0.957447 -0.50  0.000000   \n9989  -1.0  0.0   0.0  0.870566  0.113595  0.627955 -0.297872 -0.50  3.666667   \n9990  -1.0  1.0   0.0 -0.666353 -0.821265 -0.897368 -0.574468  0.25 -0.666667   \n9991   0.0  2.0   0.0 -0.642048  1.190344 -0.387783 -0.723404 -0.50  0.000000   \n9992   0.0  0.0  19.0 -0.163392  0.648342  0.129135 -0.595745 -0.25 -0.333333   \n9993  -1.0  0.0   0.0 -0.036874 -0.266021 -0.837918  0.234043  0.00 -0.666667   \n9994   0.0  0.0   0.0 -0.593316  1.325179 -1.035469  0.978723  0.75  0.000000   \n9995   0.0  0.0   0.0 -0.102637  0.172592  1.031806  0.340426  0.00  1.000000   \n9996   0.0  0.0   0.0 -0.036789  2.676727  0.817971  1.021277 -0.75  1.333333   \n9997  -1.0  8.0   0.0  0.094488 -0.797613  0.047754 -0.851064  0.75  1.000000   \n9998  -1.0  0.0   0.0 -0.193568  1.226373  0.519625 -1.042553  0.75  4.333333   \n9999  -1.0  2.0  20.0 -0.861491  0.172619  1.497792  0.829787  0.75 -0.666667   \n10000  0.0  0.0   0.0 -0.810367 -0.270522 -0.299552 -0.340426  0.00  0.333333   \n10001  0.0  0.0  29.0  0.653700  0.545622  0.780507  0.659574  0.00  0.000000   \n10002  0.0  0.0   0.0  0.069948 -0.544182  0.869789  0.276596 -0.75  2.000000   \n10003  0.0  0.0   0.0  1.391849  0.167536 -0.144341  0.276596  0.00  0.000000   \n10004  0.0  0.0   0.0  0.691820  0.230597  0.601577  0.489362  1.50  1.000000   \n\n             9         10   11   12   13   14   15   16  \n0      0.593728  0.593728  0.0  0.0  1.0 -1.0  1.0  0.0  \n1     -0.481966 -0.481966  0.0  1.0  0.0  0.0  0.0  0.0  \n2     -0.967432 -0.967432  0.0  1.0  0.0  0.0  0.0  0.0  \n3     -0.835825 -0.835825  1.0  0.0  0.0  0.0  0.0  0.0  \n4     -0.835349 -0.835349  0.0  1.0  0.0 -1.0  1.0  0.0  \n5      0.121698  0.121698  1.0  0.0  0.0 -1.0  1.0  0.0  \n6     -0.997924 -0.997924  0.0  0.0  1.0  0.0  0.0  0.0  \n7     -0.978510 -0.978510  1.0  0.0  0.0 -1.0  0.0  1.0  \n8     -0.294820 -0.294820  0.0  0.0  1.0  0.0  0.0  0.0  \n9      0.609188  0.609188  0.0  1.0  0.0  0.0  0.0  0.0  \n10    -0.238339 -0.238339  0.0  0.0  1.0 -1.0  1.0  0.0  \n11    -0.851660 -0.851660  1.0  0.0  0.0 -1.0  1.0  0.0  \n12     0.205148  0.205148  1.0  0.0  0.0  0.0  0.0  0.0  \n13    -0.205697 -0.205697  1.0  0.0  0.0 -1.0  1.0  0.0  \n14    -0.763437 -0.763437  0.0  1.0  0.0  0.0  0.0  0.0  \n15    -0.836399 -0.836399  0.0  0.0  1.0  0.0  0.0  0.0  \n16     0.239120  0.239120  1.0  0.0  0.0 -1.0  0.0  1.0  \n17     0.242081  0.242081  0.0  1.0  0.0 -1.0  0.0  1.0  \n18    -0.989084 -0.989084  0.0  0.0  1.0 -1.0  1.0  0.0  \n19    -0.850655 -0.850655  0.0  1.0  0.0  0.0  0.0  0.0  \n20    -0.159717 -0.159717  1.0  0.0  0.0  0.0  0.0  0.0  \n21    -0.149866 -0.149866  0.0  1.0  0.0 -1.0  1.0  0.0  \n22     0.344803  0.344803  0.0  0.0  1.0  0.0  0.0  0.0  \n23    -0.752482 -0.752482  0.0  1.0  0.0  0.0  0.0  0.0  \n24    -0.181791 -0.181791  0.0  0.0  1.0 -1.0  1.0  0.0  \n25    -0.113146 -0.113146  1.0  0.0  0.0  0.0  0.0  0.0  \n26    -0.650589 -0.650589  0.0  1.0  0.0 -1.0  1.0  0.0  \n27    -0.542415 -0.542415  1.0  0.0  0.0  0.0  0.0  0.0  \n28     0.739641  0.739641  1.0  0.0  0.0  0.0  0.0  0.0  \n29     0.114452  0.114452  0.0  0.0  1.0  0.0  0.0  0.0  \n...         ...       ...  ...  ...  ...  ...  ...  ...  \n9975  -0.532880 -0.532880  0.0  1.0  0.0  0.0  0.0  0.0  \n9976  -0.072012 -0.072012  0.0  1.0  0.0  0.0  0.0  0.0  \n9977   0.810673  0.810673  0.0  1.0  0.0  0.0  0.0  0.0  \n9978  -1.006671 -1.006671  0.0  1.0  0.0 -1.0  0.0  1.0  \n9979  -0.028863 -0.028863  1.0  0.0  0.0 -1.0  1.0  0.0  \n9980  -0.800074 -0.800074  0.0  1.0  0.0 -1.0  0.0  1.0  \n9981  -0.733955 -0.733955  0.0  0.0  1.0  0.0  0.0  0.0  \n9982   0.182003  0.182003  1.0  0.0  0.0 -1.0  0.0  1.0  \n9983   0.553972  0.553972  0.0  0.0  1.0  0.0  0.0  0.0  \n9984   1.010028  1.010028  0.0  0.0  1.0 -1.0  1.0  0.0  \n9985  -0.326969 -0.326969  1.0  0.0  0.0 -1.0  0.0  1.0  \n9986   0.702999  0.702999  0.0  0.0  1.0 -1.0  1.0  0.0  \n9987  -0.687078 -0.687078  0.0  0.0  1.0  0.0  0.0  0.0  \n9988   0.052082  0.052082  1.0  0.0  0.0 -1.0  1.0  0.0  \n9989  -0.198146 -0.198146  0.0  0.0  1.0  0.0  0.0  0.0  \n9990  -0.711860 -0.711860  0.0  0.0  1.0  0.0  0.0  0.0  \n9991   0.086029  0.086029  0.0  1.0  0.0  0.0  0.0  0.0  \n9992  -0.063321 -0.063321  0.0  0.0  1.0  0.0  0.0  0.0  \n9993  -0.743157 -0.743157  1.0  0.0  0.0 -1.0  1.0  0.0  \n9994   0.812190  0.812190  0.0  1.0  0.0 -1.0  1.0  0.0  \n9995   0.546955  0.546955  0.0  1.0  0.0 -1.0  1.0  0.0  \n9996   0.817335  0.817335  0.0  1.0  0.0  0.0  0.0  0.0  \n9997   0.218711  0.218711  0.0  1.0  0.0 -1.0  1.0  0.0  \n9998   0.775368  0.775368  0.0  1.0  0.0 -1.0  1.0  0.0  \n9999   0.945228  0.945228  1.0  0.0  0.0 -1.0  0.0  1.0  \n10000 -0.920674 -0.920674  0.0  1.0  0.0  0.0  0.0  0.0  \n10001  0.588684  0.588684  0.0  1.0  0.0  0.0  0.0  0.0  \n10002  0.838562  0.838562  0.0  0.0  1.0  0.0  0.0  0.0  \n10003  0.274096  0.274096  0.0  1.0  0.0  0.0  0.0  0.0  \n10004 -0.453527 -0.453527  0.0  1.0  0.0  0.0  0.0  0.0  \n\n[10005 rows x 17 columns]\n"
                }
            ]
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "source": "#Define independent & dependent variable\nx = S_array[:,1:16]\ny = S_array[:,0]", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "### Comparing 6 Models \nRunning the example provides a list of each algorithm short name, the mean accuracy and\nthe standard deviation accuracy.\nThe example also provides a box and whisker plot showing the spread of the accuracy scores\nacross each cross-validation fold for each algorithm.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 44, 
            "cell_type": "code", 
            "source": "# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n kfold = KFold(n_splits=10, random_state=7)\n cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "LR: 0.705246 (0.012724)\nLDA: 0.705746 (0.012613)\nKNN: 0.781607 (0.010065)\nCART: 0.922441 (0.009794)\nNB: 0.710943 (0.012664)\nSVM: 0.830684 (0.008038)\n"
                }, 
                {
                    "data": {
                        "text/plain": "<matplotlib.figure.Figure at 0x7f3a20152f50>", 
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAESCAYAAAAG+ZUXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neECOWHAAAAAASUVORK5CYII=\n"
                    }, 
                    "metadata": {}, 
                    "output_type": "display_data"
                }
            ]
        }, 
        {
            "source": "## Pipeline & Workflow Automation\nAn easy trap to fall into in applied machine learning is leaking data from your training dataset\nto your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak\nknowledge of the whole training dataset to the algorithm. For example, preparing your data\nusing normalization or standardization on the entire training dataset before learning would not\nbe a valid test because the training dataset would have been in\nuenced by the scale of the data\nin the test set.\nPipelines help you prevent data leakage in your test harness by ensuring that data preparation\nlike standardization is constrained to each fold of your cross-validation procedure. The example\nbelow demonstrates this important data preparation and model evaluation work\now on the\nPima Indians onset of diabetes dataset. The pipeline is de\fned with two steps:\n\n**1. Standardize the data.**\n\n**2. Learn a Linear Discriminant Analysis model.**\n\nThe pipeline is then evaluated using 10-fold cross-validation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "### Basic\nNotice how we create a Python list of steps that are provided to the Pipeline for process\nthe data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its\nentirety by the k-fold cross-validation procedure. Running the example provides a summary of\naccuracy of the setup on the dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Pipeline\nNotice how we create a Python list of steps that are provided to the Pipeline for process\nthe data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its\nentirety by the k-fold cross-validation procedure. Running the example provides a summary of\naccuracy of the setup on the dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 45, 
            "cell_type": "code", 
            "source": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.705745554446\n"
                }
            ]
        }, 
        {
            "source": "### Feature Extraction and Modeling Pipeline\nFeature extraction is another procedure that is susceptible to data leakage. Like data preparation,\nfeature extraction procedures must be restricted to the data in your training dataset. The\npipeline provides a handy tool called the FeatureUnion which allows the results of multiple\nfeature selection and extraction procedures to be combined into a larger dataset on which a\nmodel can be trained. Importantly, all the feature extraction and the feature union occurs\nwithin each fold of the cross-validation procedure. The example below demonstrates the pipeline\nde\fned with four steps:\n\n** 1. Feature Extraction with Principal Component Analysis (3 features). **\n\n** 2. Feature Extraction with Statistical Selection (6 features). **\n\n** 3. Feature Union. **\n\n** 4. Learn a Logistic Regression Model. **\n\n\nThe pipeline is then evaluated using 10-fold cross-validation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 46, 
            "cell_type": "code", 
            "source": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.707244955045\n"
                }
            ]
        }, 
        {
            "source": "# Improve Performance with Ensembles\nEnsembles can give you a boost in accuracy on your dataset. In this chapter you will discover\nhow you can create some of the most powerful types of ensembles in Python using scikit-learn.\nThis lesson will step you through Boosting, Bagging and Majority Voting and show you how you\ncan continue to ratchet up the accuracy of the models on your own datasets. After completing\nthis lesson you will know:\n1. How to use bagging ensemble methods such as bagged decision trees, random forest and\nextra trees.\n2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n3. How to use voting ensemble methods to combine the predictions from multiple algorithms.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Combine Models Into Ensemble Predictions\nThe three most popular methods for combining the predictions from di\u000berent models are:\n\n- **Bagging**. Building multiple models (typically of the same type) from di\u000berent subsamples of the training dataset.\n\n- **Boosting**. Building multiple models (typically of the same type) each of which learns to\n\fx the prediction errors of a prior model in the sequence of models.\n\n- **Voting**. Building multiple models (typically of di\u000bering types) and simple statistics (like calculating the mean) are used to combine predictions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Bagging Algorithms\nBootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n(with replacement) and training a model for each sample. The \fnal output prediction is averaged\nacross the predictions of all of the sub-models. The three bagging models covered in this section\nare as follows:\n1. Bagged Decision Trees.\n\nBagging performs best with algorithms that have high variance. A popular example are\ndecision trees, often constructed without pruning.\n2. Random Forest.\n\nRandom Forests is an extension of bagged decision trees. Samples of the training dataset are\ntaken with replacement, but the trees are constructed in a way that reduces the correlation\nbetween individual classi\fers. Speci\fcally, rather than greedily choosing the best split point in\nthe construction of each tree, only a random subset of features are considered for each split.\n3. Extra Trees.\n\nExtra Trees are another modi\fcation of bagging where random trees are constructed from\nsamples of the training dataset. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 48, 
            "cell_type": "code", 
            "source": "#BaggingTree\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nseed = 7\nkfold = KFold(n_splits=10, random_state=seed)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.950326473526\n"
                }
            ]
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "source": "#RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nnum_trees = 100\nmax_features = 3\nkfold = KFold(n_splits=10, random_state=7)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.945828871129\n"
                }
            ]
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "source": "#ExtraTree\nfrom sklearn.ensemble import ExtraTreesClassifier\nnum_trees = 100\nmax_features = 7\nkfold = KFold(n_splits=10, random_state=7)\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.960420979021\n"
                }
            ]
        }, 
        {
            "source": "## Boosting Algorithms\nBoosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\nof the models before them in the sequence. Once created, the models make predictions which\nmay be weighted by their demonstrated accuracy and the results are combined to create a \fnal\noutput prediction. The two most common boosting ensemble machine learning algorithms are:\n\n\u0088$$ AdaBoost.$$\n\u0088$$ Stochastic Gradient Boosting. $$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## AdaBoost\nAdaBoost was perhaps the \frst successful boosting ensemble algorithm. It generally works by\nweighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing the\nalgorithm to pay less attention to them in the construction of subsequent models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "source": "from sklearn.ensemble import AdaBoostClassifier\nnum_trees = 30\nseed=7\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.864467232767\n"
                }
            ]
        }, 
        {
            "source": "## Stochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\nsophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\nthe best techniques available for improving performance via ensembles.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "source": "from sklearn.ensemble import GradientBoostingClassifier\nseed = 7\nnum_trees = 100\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, x, y, cv=kfold)\nprint(results.mean())", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.907346853147\n"
                }
            ]
        }, 
        {
            "source": "# Voting Ensemble\nVoting is one of the simplest ways of combining the predictions from multiple machine learning\nalgorithms. It works by \frst creating two or more standalone models from your training dataset.\nA Voting Classi\fer can then be used to wrap your models and average the predictions of the\nsub-models when asked to make predictions for new data. The predictions of the sub-models can\nbe weighted, but specifying the weights for classi\fers manually or even heuristically is di\u000ecult.\nMore advanced methods can learn how to best weight the predictions from sub-models, but this\nis called stacking (stacked aggregation) and is currently not provided in scikit-learn. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "source": "from sklearn.ensemble import VotingClassifier\nkfold = KFold(n_splits=10, random_state=7)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, x, y, cv=kfold)\nprint(results.mean())\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels('Voting')\npyplot.show()", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.833283116883\n"
                }, 
                {
                    "data": {
                        "text/plain": "<matplotlib.figure.Figure at 0x7f3a0056bfd0>", 
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqUcMfUnqEUNfknrE0JekHjH0JalH/j9o21lW6DA8RQAAAABJRU5ErkJggg==\n"
                    }, 
                    "metadata": {}, 
                    "output_type": "display_data"
                }
            ]
        }, 
        {
            "source": "# Improve Performance with Algorithm Tuning\nAlgorithm tuning is a \fnal step in the process of applied machine learning before \fnalizing your\nmodel. It is sometimes called hyperparameter optimization where the algorithm parameters\nare referred to as hyperparameters, whereas the coe\u000ecients found by the machine learning\nalgorithm itself are referred to as parameters. Optimization suggests the search-nature of the\nproblem. Phrased as a search problem, you can use di\u000berent search strategies to \fnd a good and\nrobust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn\nprovides two simple methods for algorithm parameter tuning:\n\u0088 \n\n**Grid Search Parameter Tuning.**\n\n\u0088**Random Search Parameter Tuning.**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##  Grid Search Parameter Tuning\nGrid search is an approach to parameter tuning that will methodically build and evaluate a\nmodel for each combination of algorithm parameters speci\fed in a grid. You can perform a grid", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 63, 
            "cell_type": "code", 
            "source": "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nalphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(x, y)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)\n#Running the example lists out the optimal score achieved and the set of parameters in the\n#grid that achieved that score. In this case the alpha value of 1.0.", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.0675642367707\n1.0\n"
                }
            ]
        }, 
        {
            "source": "## Random Search Parameter Tuning\nRandom search is an approach to parameter tuning that will sample algorithm parameters from\na random distribution (i.e. uniform) for a \fxed number of iterations. A model is constructed\nand evaluated for each combination of parameters chosen. The example below evaluates\ndi\u000berent random alpha values between 0 and 1 for the Ridge Regression algorithm on the\nstandard diabetes dataset. A total of 100 iterations are performed with uniformly random alpha\nvalues selected in the range between 0 and 1 (the range that alpha values can take).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 65, 
            "cell_type": "code", 
            "source": "from scipy.stats import uniform\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_grid = {'alpha': uniform()}\nmodel = Ridge()\nrsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100,\nrandom_state=7)\nrsearch.fit(x, y)\nprint(rsearch.best_score_)\nprint(rsearch.best_estimator_.alpha)\n#Running the example produces results much like those in the grid search example above. An\n#optimal alpha value near 1.0 is discovered.", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.0675642005572\n0.977989511997\n"
                }
            ]
        }, 
        {
            "source": "# Final Step: Save & Load Models\nFinding an accurate machine learning model is not the end of the project. In this chapter you\nwill discover how to save and load your machine learning model in Python using scikit-learn.\nThis allows you to save your model to \fle and load it later in order to make predictions. After\ncompleting this lesson you will know:\n1. The importance of serializing models for reuse.\n2. How to use pickle to serialize and deserialize machine learning models.\n3. How to use Joblib to serialize and deserialize machine learning models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Pickle\nPickle is the standard way of serializing objects in Python. You can use the pickle1 operation\nto serialize your machine learning algorithms and save the serialized format to a \fle. Later you\ncan load this \fle to deserialize your model and use it to make new predictions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom pickle import dump\nfrom pickle import load\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "execution_count": 71, 
            "cell_type": "code", 
            "source": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n# Fit the model on 33%\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, open(filename, 'wb'))\n# some time later...\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\nresult = loaded_model.score(x_test, y_test)\nprint(result)\n#Running the example saves the model to finalized model.sav in your local working\n#directory. Load the saved model and evaluating it provides an estimate of accuracy of the model\n#on unseen data.", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.711689884918\n"
                }
            ]
        }, 
        {
            "source": "## Finalize Your Model with Joblib\nThe Joblib2 library is part of the SciPy ecosystem and provides utilities for pipelining Python\njobs. It provides utilities for saving and loading Python objects that make use of NumPy data\nstructures, e\u000eciently3. This can be useful for some machine learning algorithms that require a\nlot of parameters or store the entire dataset (e.g. k-Nearest Neighbors).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 72, 
            "cell_type": "code", 
            "source": "from sklearn.externals.joblib import dump\nfrom sklearn.externals.joblib import load\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n# Fit the model on 33%\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, open(filename, 'wb'))\n# some time later...\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\nresult = loaded_model.score(x_test, y_test)\nprint(result)\n#Running the example saves the model to finalized model.sav in your local working\n#directory. Load the saved model and evaluating it provides an estimate of accuracy of the model\n#on unseen data.", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.711689884918\n"
                }
            ]
        }, 
        {
            "source": "## Tips for Finalizing Your Model\nThis section lists some important considerations when \fnalizing your machine learning models.\n\n** - Python Version. **\n\nTake note of the Python version. You almost certainly require the\nsame major (and maybe minor) version of Python used to serialize the model when you\nlater load it and deserialize it.\n\n** - Library Versions. **\n\nThe version of all major libraries used in your machine learning\nproject almost certainly need to be the same when deserializing a saved model. This is\nnot limited to the version of NumPy and the version of scikit-learn.\n\n** - Manual Serialization. **\n\nYou might like to manually output the parameters of your\nlearned model so that you can use them directly in scikit-learn or another platform in\nthe future. Often the techniques used internally by machine learning algorithms to make\npredictions are a lot simpler than those used to learn the parameters and can be easy to\nimplement in custom code that you have control over.\n\nTake note of the version so that you can re-create the environment if for some reason you\ncannot reload your model on another machine or another platform at a later time.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "version": "2.7.11", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat_minor": 1
}
